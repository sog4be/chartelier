{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-OSS-20B on Google Colab for Chartelier\n",
    "\n",
    "This notebook demonstrates running Chartelier with GPT-OSS-20B on Google Colab using an A100 GPU.\n",
    "\n",
    "## Prerequisites\n",
    "- Google Colab Pro+ account (for A100 access)\n",
    "- GPU runtime enabled (Runtime -> Change runtime type -> A100 GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Clone Repository and Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the Chartelier repository\n",
    "!git clone https://github.com/sog4be/chartelier.git\n",
    "%cd chartelier\n",
    "\n",
    "# Check current branch (should be feature/gpt-oss-20b-colab-support)\n",
    "!git checkout feature/gpt-oss-20b-colab-support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the setup script\n",
    "!python colab/setup_gpt_oss.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Start vLLM Server (Run in Background)\n",
    "\n",
    "**Important**: This cell will keep running. Start it and then proceed to the next cells while it runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start vLLM server in the background\n",
    "# This will download the model (~14GB) on first run\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Start server in background\n",
    "server_process = subprocess.Popen(\n",
    "    [\"python\", \"colab/start_vllm_server.py\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting vLLM server...\")\n",
    "print(\"‚è≥ This may take 2-5 minutes on first run while downloading the model\")\n",
    "\n",
    "# Wait a bit for server to start\n",
    "time.sleep(10)\n",
    "\n",
    "# Check if server is starting\n",
    "import requests\n",
    "\n",
    "max_attempts = 60  # 5 minutes max\n",
    "attempt = 0\n",
    "\n",
    "while attempt < max_attempts:\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:8000/health\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(\"\\n‚úÖ vLLM server is ready!\")\n",
    "            break\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if attempt % 6 == 0:  # Print status every 30 seconds\n",
    "        print(f\"‚è≥ Waiting for server... ({attempt * 5}s elapsed)\")\n",
    "\n",
    "    time.sleep(5)\n",
    "    attempt += 1\n",
    "\n",
    "if attempt >= max_attempts:\n",
    "    print(\"‚ùå Server failed to start. Check the logs above.\")\n",
    "else:\n",
    "    # Check if model is loaded\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:8000/v1/models\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json().get(\"data\", [])\n",
    "            if models:\n",
    "                print(f\"‚úÖ Model loaded: {models[0]['id']}\")\n",
    "                print(\"\\nüéâ You can now run the test in the next cell!\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Could not verify model loading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test the Setup\n",
    "\n",
    "First, let's verify the server is working with a simple test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test of the vLLM server\n",
    "import json\n",
    "\n",
    "import requests\n",
    "\n",
    "# Test the OpenAI-compatible endpoint\n",
    "url = \"http://localhost:8000/v1/chat/completions\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "data = {\n",
    "    \"model\": \"openai/gpt-oss-20b\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n",
    "    ],\n",
    "    \"temperature\": 0.0,\n",
    "    \"max_tokens\": 50,\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(url, headers=headers, json=data, timeout=30)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(\"‚úÖ Server test successful!\")\n",
    "        print(f\"Response: {result['choices'][0]['message']['content']}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Server returned error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to connect to server: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Chartelier E2E Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the test script to use the Colab environment\n",
    "import os\n",
    "\n",
    "# Set environment variables for Chartelier\n",
    "os.environ[\"CHARTELIER_LLM_MODEL\"] = \"openai/gpt-oss-20b\"\n",
    "os.environ[\"CHARTELIER_LLM_API_BASE\"] = \"http://localhost:8000/v1\"\n",
    "os.environ[\"CHARTELIER_LLM_API_KEY\"] = \"dummy\"  # vLLM doesn't need a real key for local\n",
    "os.environ[\"CHARTELIER_LLM_TIMEOUT\"] = \"30\"\n",
    "\n",
    "print(\"Environment configured:\")\n",
    "print(f\"  Model: {os.environ['CHARTELIER_LLM_MODEL']}\")\n",
    "print(f\"  API Base: {os.environ['CHARTELIER_LLM_API_BASE']}\")\n",
    "print(f\"  Timeout: {os.environ['CHARTELIER_LLM_TIMEOUT']}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the E2E test\n",
    "!python temp/test_e2e.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: View Generated Chart\n",
    "\n",
    "If the test was successful, display the generated chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated SVG chart\n",
    "import os\n",
    "\n",
    "from IPython.display import SVG, display\n",
    "\n",
    "output_path = \"temp/output.svg\"\n",
    "if os.path.exists(output_path):\n",
    "    print(\"üìä Generated Chart:\")\n",
    "    display(SVG(filename=output_path))\n",
    "else:\n",
    "    print(\"‚ùå No output file found. The test may have failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Custom Visualization Test\n",
    "\n",
    "Try creating your own visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom visualization test\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"src\")\n",
    "\n",
    "from chartelier.interfaces.mcp.handler import MCPHandler\n",
    "from chartelier.interfaces.mcp.protocol import JSONRPCRequest, MCPMethod\n",
    "\n",
    "# Sample data - different from the default test\n",
    "custom_data = \"\"\"date,temperature,city\n",
    "2024-01-01,5,Tokyo\n",
    "2024-01-02,7,Tokyo\n",
    "2024-01-03,6,Tokyo\n",
    "2024-01-04,8,Tokyo\n",
    "2024-01-01,10,Osaka\n",
    "2024-01-02,12,Osaka\n",
    "2024-01-03,11,Osaka\n",
    "2024-01-04,13,Osaka\"\"\"\n",
    "\n",
    "# Create request\n",
    "handler = MCPHandler()\n",
    "request = JSONRPCRequest(\n",
    "    id=2,\n",
    "    method=MCPMethod.TOOLS_CALL,\n",
    "    params={\n",
    "        \"name\": \"chartelier_visualize\",\n",
    "        \"arguments\": {\n",
    "            \"data\": custom_data,\n",
    "            \"query\": \"Compare daily temperature trends between Tokyo and Osaka\",\n",
    "            \"options\": {\n",
    "                \"format\": \"svg\",\n",
    "                \"width\": 800,\n",
    "                \"height\": 600,\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"üé® Creating custom visualization...\")\n",
    "print(\"Query: 'Compare daily temperature trends between Tokyo and Osaka'\")\n",
    "\n",
    "try:\n",
    "    response_str = handler.handle_message(json.dumps(request.model_dump()))\n",
    "    response = json.loads(response_str)\n",
    "\n",
    "    if response.get(\"result\", {}).get(\"isError\"):\n",
    "        print(\"‚ùå Visualization failed\")\n",
    "        print(response[\"result\"][\"content\"][0][\"text\"])\n",
    "    else:\n",
    "        print(\"‚úÖ Visualization successful!\")\n",
    "\n",
    "        # Save and display the result\n",
    "        if \"content\" in response[\"result\"] and len(response[\"result\"][\"content\"]) > 0:\n",
    "            content = response[\"result\"][\"content\"][0]\n",
    "            if content[\"type\"] == \"image\" and \"svg\" in content.get(\"mimeType\", \"\"):\n",
    "                svg_data = content[\"data\"]\n",
    "\n",
    "                # Save to file\n",
    "                with open(\"temp/custom_output.svg\", \"w\") as f:\n",
    "                    f.write(svg_data)\n",
    "\n",
    "                # Display\n",
    "                from IPython.display import SVG, display\n",
    "\n",
    "                display(SVG(data=svg_data))\n",
    "\n",
    "                # Show metadata\n",
    "                if \"structuredContent\" in response[\"result\"]:\n",
    "                    metadata = response[\"result\"][\"structuredContent\"].get(\"metadata\", {})\n",
    "                    print(\"\\nüìä Metadata:\")\n",
    "                    print(f\"   Pattern: {metadata.get('pattern_id')}\")\n",
    "                    print(f\"   Template: {metadata.get('template_id')}\")\n",
    "                    if metadata.get(\"processing_time_ms\"):\n",
    "                        print(f\"   Processing time: {metadata['processing_time_ms']}ms\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Stop the vLLM server when done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the vLLM server\n",
    "try:\n",
    "    server_process.terminate()\n",
    "    server_process.wait(timeout=5)\n",
    "    print(\"‚úÖ vLLM server stopped\")\n",
    "except:\n",
    "    print(\"Server process was not running or already stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "1. **No GPU available**: Make sure you've selected GPU runtime (Runtime -> Change runtime type -> GPU)\n",
    "2. **Out of memory**: The A100 40GB should be sufficient, but if you get OOM errors, try restarting the runtime\n",
    "3. **Model download slow**: First run downloads ~14GB model. This is normal and will be cached for future runs\n",
    "4. **Server not starting**: Check the server logs in the cell output for specific errors\n",
    "5. **Connection refused**: Make sure the vLLM server cell is still running"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-OSS-20B on Google Colab for Chartelier\n",
    "\n",
    "This notebook demonstrates running Chartelier with GPT-OSS-20B on Google Colab using an A100 GPU.\n",
    "\n",
    "## Prerequisites\n",
    "- Google Colab Pro+ account (for A100 access)\n",
    "- GPU runtime enabled (Runtime -> Change runtime type -> A100 GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Clone Repository and Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the Chartelier repository\n",
    "!git clone https://github.com/sog4be/chartelier.git\n",
    "%cd chartelier\n",
    "\n",
    "# Check current branch (should be feature/gpt-oss-20b-colab-support)\n",
    "!git checkout feature/gpt-oss-20b-colab-support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the setup script\n",
    "!python colab/setup_gpt_oss.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Start vLLM Server (Run in Background)\n",
    "\n",
    "**Important**: This cell will keep running. Start it and then proceed to the next cells while it runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start vLLM server in the background\n",
    "# This will download the model (~14GB) on first run\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Start server in background\n",
    "server_process = subprocess.Popen(\n",
    "    [\"python\", \"colab/start_vllm_server.py\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting vLLM server...\")\n",
    "print(\"‚è≥ This may take 2-5 minutes on first run while downloading the model\")\n",
    "\n",
    "# Wait a bit for server to start\n",
    "time.sleep(10)\n",
    "\n",
    "# Check if server is starting\n",
    "import requests\n",
    "\n",
    "max_attempts = 60  # 5 minutes max\n",
    "attempt = 0\n",
    "\n",
    "while attempt < max_attempts:\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:8000/health\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(\"\\n‚úÖ vLLM server is ready!\")\n",
    "            break\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if attempt % 6 == 0:  # Print status every 30 seconds\n",
    "        print(f\"‚è≥ Waiting for server... ({attempt * 5}s elapsed)\")\n",
    "\n",
    "    time.sleep(5)\n",
    "    attempt += 1\n",
    "\n",
    "if attempt >= max_attempts:\n",
    "    print(\"‚ùå Server failed to start. Check the logs above.\")\n",
    "else:\n",
    "    # Check if model is loaded\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:8000/v1/models\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json().get(\"data\", [])\n",
    "            if models:\n",
    "                print(f\"‚úÖ Model loaded: {models[0]['id']}\")\n",
    "                print(\"\\nüéâ You can now run the test in the next cell!\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Could not verify model loading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test the Setup\n",
    "\n",
    "First, let's verify the server is working with a simple test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test of the vLLM server\n",
    "import json\n",
    "\n",
    "import requests\n",
    "\n",
    "# Test the OpenAI-compatible endpoint\n",
    "url = \"http://localhost:8000/v1/chat/completions\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "data = {\n",
    "    \"model\": \"openai/gpt-oss-20b\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n",
    "    ],\n",
    "    \"temperature\": 0.0,\n",
    "    \"max_tokens\": 50,\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(url, headers=headers, json=data, timeout=30)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(\"‚úÖ Server test successful!\")\n",
    "        print(f\"Response: {result['choices'][0]['message']['content']}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Server returned error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to connect to server: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Chartelier E2E Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the test script to use the Colab environment\n",
    "import os\n",
    "\n",
    "# Set environment variables for Chartelier\n",
    "os.environ[\"CHARTELIER_LLM_MODEL\"] = \"openai/gpt-oss-20b\"\n",
    "os.environ[\"CHARTELIER_LLM_API_BASE\"] = \"http://localhost:8000/v1\"\n",
    "os.environ[\"CHARTELIER_LLM_API_KEY\"] = \"dummy\"  # vLLM doesn't need a real key for local\n",
    "os.environ[\"CHARTELIER_LLM_TIMEOUT\"] = \"30\"\n",
    "\n",
    "print(\"Environment configured:\")\n",
    "print(f\"  Model: {os.environ['CHARTELIER_LLM_MODEL']}\")\n",
    "print(f\"  API Base: {os.environ['CHARTELIER_LLM_API_BASE']}\")\n",
    "print(f\"  Timeout: {os.environ['CHARTELIER_LLM_TIMEOUT']}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chartelier„ÉÜ„Çπ„ÉàÂÆüË°åÔºàÁõ¥Êé•ÂÆüË°åÁâàÔºâ\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Áí∞Â¢ÉÂ§âÊï∞„ÇíÁ¢∫Ë™çÔºàÊó¢„Å´Ë®≠ÂÆöÊ∏à„Åø„ÅÆ„ÅØ„ÅöÔºâ\n",
    "print(\"Environment variables:\")\n",
    "print(f\"  Model: {os.environ.get('CHARTELIER_LLM_MODEL', 'Not set')}\")\n",
    "print(f\"  API Base: {os.environ.get('CHARTELIER_LLM_API_BASE', 'Not set')}\")\n",
    "print(f\"  Timeout: {os.environ.get('CHARTELIER_LLM_TIMEOUT', 'Not set')}s\")\n",
    "print()\n",
    "\n",
    "# src„Çí„Éë„Çπ„Å´ËøΩÂä†\n",
    "sys.path.insert(0, \"/content/chartelier/src\")\n",
    "\n",
    "from chartelier.interfaces.mcp.handler import MCPHandler\n",
    "from chartelier.interfaces.mcp.protocol import JSONRPCRequest, MCPMethod\n",
    "from chartelier.infra.llm_client import LLMSettings\n",
    "\n",
    "\n",
    "def test_chartelier():\n",
    "    \"\"\"Chartelier„ÅÆ„Ç®„É≥„Éâ„ÉÑ„Éº„Ç®„É≥„Éâ„ÉÜ„Çπ„Éà\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üß™ Chartelier End-to-End Test\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Ë®≠ÂÆöÁ¢∫Ë™ç\n",
    "    settings = LLMSettings()\n",
    "    print(f\"\\n‚úÖ Configuration:\")\n",
    "    print(f\"   Model: {settings.model}\")\n",
    "    print(f\"   API Base: {settings.api_base}\")\n",
    "    print(f\"   Timeout: {settings.timeout}s\")\n",
    "\n",
    "    # MCP„Éè„É≥„Éâ„É©„Éº‰ΩúÊàê\n",
    "    handler = MCPHandler()\n",
    "    print(\"‚úÖ MCP handler created\")\n",
    "\n",
    "    # „ÉÜ„Çπ„Éà„Éá„Éº„Çø\n",
    "    csv_data = \"\"\"month,sales,category\n",
    "2024-01,1000,Product A\n",
    "2024-02,1200,Product A\n",
    "2024-03,1100,Product A\n",
    "2024-04,1300,Product A\n",
    "2024-01,800,Product B\n",
    "2024-02,900,Product B\n",
    "2024-03,950,Product B\n",
    "2024-04,1050,Product B\"\"\"\n",
    "\n",
    "    # ÂèØË¶ñÂåñ„É™„ÇØ„Ç®„Çπ„Éà‰ΩúÊàê\n",
    "    request = JSONRPCRequest(\n",
    "        id=1,\n",
    "        method=MCPMethod.TOOLS_CALL,\n",
    "        params={\n",
    "            \"name\": \"chartelier_visualize\",\n",
    "            \"arguments\": {\n",
    "                \"data\": csv_data,\n",
    "                \"query\": \"Show monthly sales trends for Product A and Product B as a line chart\",\n",
    "                \"options\": {\n",
    "                    \"format\": \"svg\",\n",
    "                    \"width\": 800,\n",
    "                    \"height\": 600,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(\"\\n‚úÖ Request prepared\")\n",
    "    print(f\"üìà Data: {len(csv_data.splitlines()) - 1} rows\")\n",
    "    print(f\"üìù Query: 'Show monthly sales trends for Product A and Product B'\")\n",
    "    print(f\"üé® Format: SVG (800x600)\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚ö†Ô∏è  Ready to send request to LLM\")\n",
    "    print(f\"üöÄ Using local vLLM server with {settings.model}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # „É™„ÇØ„Ç®„Çπ„ÉàÂá¶ÁêÜ\n",
    "    print(\"\\n‚è≥ Processing visualization request...\")\n",
    "    print(\"   Phase 1: Data validation\")\n",
    "    print(\"   Phase 2: Pattern selection (LLM)\")\n",
    "    print(\"   Phase 3: Chart selection (LLM)\")\n",
    "    print(\"   Phase 4: Data processing\")\n",
    "    print(\"   Phase 5: Data mapping (LLM)\")\n",
    "    print(\"   Phase 6: Chart building\")\n",
    "\n",
    "    try:\n",
    "        response_str = handler.handle_message(json.dumps(request.model_dump()))\n",
    "        response = json.loads(response_str)\n",
    "\n",
    "        if response.get(\"result\", {}).get(\"isError\"):\n",
    "            print(\"\\n‚ùå Visualization failed\")\n",
    "            error_msg = response[\"result\"][\"content\"][0][\"text\"]\n",
    "            print(f\"   Error: {error_msg}\")\n",
    "\n",
    "            if \"structuredContent\" in response[\"result\"]:\n",
    "                error = response[\"result\"][\"structuredContent\"].get(\"error\", {})\n",
    "                print(f\"   Code: {error.get('code')}\")\n",
    "                if error.get(\"hint\"):\n",
    "                    print(f\"   Hint: {error.get('hint')}\")\n",
    "            return None\n",
    "        else:\n",
    "            print(\"\\n‚úÖ Visualization successful!\")\n",
    "\n",
    "            result = response[\"result\"]\n",
    "\n",
    "            # ÁîªÂÉè„Ç≥„É≥„ÉÜ„É≥„ÉÑÁ¢∫Ë™ç\n",
    "            if \"content\" in result and len(result[\"content\"]) > 0:\n",
    "                content = result[\"content\"][0]\n",
    "                if content[\"type\"] == \"image\":\n",
    "                    print(f\"\\nüìä Chart generated:\")\n",
    "                    print(f\"   MIME type: {content.get('mimeType', 'unknown')}\")\n",
    "                    print(f\"   Data size: {len(content.get('data', ''))} characters\")\n",
    "\n",
    "                    # „É°„Çø„Éá„Éº„ÇøË°®Á§∫\n",
    "                    if \"structuredContent\" in result and \"metadata\" in result[\"structuredContent\"]:\n",
    "                        metadata = result[\"structuredContent\"][\"metadata\"]\n",
    "                        print(f\"\\nüìä Processing metadata:\")\n",
    "                        pattern_id = metadata.get(\"pattern_id\")\n",
    "                        print(f\"   Pattern: {pattern_id} - {get_pattern_description(pattern_id)}\")\n",
    "                        print(f\"   Template: {metadata.get('template_id')}\")\n",
    "\n",
    "                        if metadata.get(\"mapping\"):\n",
    "                            print(f\"   Mapping:\")\n",
    "                            for key, value in metadata[\"mapping\"].items():\n",
    "                                print(f\"      {key}: {value}\")\n",
    "\n",
    "                        # Âá¶ÁêÜÊôÇÈñìË°®Á§∫\n",
    "                        if metadata.get(\"stats\", {}).get(\"duration_ms\"):\n",
    "                            duration = metadata[\"stats\"][\"duration_ms\"]\n",
    "                            total = duration.get(\"total\", 0)\n",
    "                            print(f\"   Processing time: {total:.0f}ms\")\n",
    "\n",
    "                    # SVG„ÇíËøî„Åô\n",
    "                    if \"svg\" in content.get(\"mimeType\", \"\"):\n",
    "                        return content[\"data\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Unexpected error: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_pattern_description(pattern_id):\n",
    "    \"\"\"„Éë„Çø„Éº„É≥ID„ÅÆË™¨Êòé„ÇíÂèñÂæó\"\"\"\n",
    "    patterns = {\n",
    "        \"P01\": \"Single time series\",\n",
    "        \"P02\": \"Category comparison\",\n",
    "        \"P03\": \"Distribution overview\",\n",
    "        \"P12\": \"Multiple time series comparison\",\n",
    "        \"P13\": \"Distribution over time\",\n",
    "        \"P21\": \"Category differences over time\",\n",
    "        \"P23\": \"Distribution comparison by category\",\n",
    "        \"P31\": \"Overall patterns over time\",\n",
    "        \"P32\": \"Distribution comparison across categories\",\n",
    "    }\n",
    "    return patterns.get(pattern_id, \"Unknown pattern\")\n",
    "\n",
    "\n",
    "# „ÉÜ„Çπ„ÉàÂÆüË°å\n",
    "svg_data = test_chartelier()\n",
    "\n",
    "# ÁµêÊûúË°®Á§∫\n",
    "if svg_data:\n",
    "    from IPython.display import SVG, display\n",
    "\n",
    "    print(\"\\nüìä Displaying generated chart:\")\n",
    "    display(SVG(data=svg_data))\n",
    "\n",
    "    # „Éï„Ç°„Ç§„É´„Å´‰øùÂ≠ò\n",
    "    output_path = Path(\"/content/output.svg\")\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(svg_data)\n",
    "    print(f\"\\nüíæ Chart saved to: {output_path}\")\n",
    "    print(\"   You can download it from the Files panel on the left\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Chart generation failed\")\n",
    "    print(\"\\nPlease check:\")\n",
    "    print(\"1. vLLM server is running (Step 2)\")\n",
    "    print(\"2. Environment variables are set (Step 4)\")\n",
    "    print(\"3. No errors in server logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: View Generated Chart\n",
    "\n",
    "If the test was successful, display the generated chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated SVG chart\n",
    "import os\n",
    "\n",
    "from IPython.display import SVG, display\n",
    "\n",
    "output_path = \"temp/output.svg\"\n",
    "if os.path.exists(output_path):\n",
    "    print(\"üìä Generated Chart:\")\n",
    "    display(SVG(filename=output_path))\n",
    "else:\n",
    "    print(\"‚ùå No output file found. The test may have failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Custom Visualization Test\n",
    "\n",
    "Try creating your own visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom visualization test\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"src\")\n",
    "\n",
    "from chartelier.interfaces.mcp.handler import MCPHandler\n",
    "from chartelier.interfaces.mcp.protocol import JSONRPCRequest, MCPMethod\n",
    "\n",
    "# Sample data - different from the default test\n",
    "custom_data = \"\"\"date,temperature,city\n",
    "2024-01-01,5,Tokyo\n",
    "2024-01-02,7,Tokyo\n",
    "2024-01-03,6,Tokyo\n",
    "2024-01-04,8,Tokyo\n",
    "2024-01-01,10,Osaka\n",
    "2024-01-02,12,Osaka\n",
    "2024-01-03,11,Osaka\n",
    "2024-01-04,13,Osaka\"\"\"\n",
    "\n",
    "# Create request\n",
    "handler = MCPHandler()\n",
    "request = JSONRPCRequest(\n",
    "    id=2,\n",
    "    method=MCPMethod.TOOLS_CALL,\n",
    "    params={\n",
    "        \"name\": \"chartelier_visualize\",\n",
    "        \"arguments\": {\n",
    "            \"data\": custom_data,\n",
    "            \"query\": \"Compare daily temperature trends between Tokyo and Osaka\",\n",
    "            \"options\": {\n",
    "                \"format\": \"svg\",\n",
    "                \"width\": 800,\n",
    "                \"height\": 600,\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"üé® Creating custom visualization...\")\n",
    "print(\"Query: 'Compare daily temperature trends between Tokyo and Osaka'\")\n",
    "\n",
    "try:\n",
    "    response_str = handler.handle_message(json.dumps(request.model_dump()))\n",
    "    response = json.loads(response_str)\n",
    "\n",
    "    if response.get(\"result\", {}).get(\"isError\"):\n",
    "        print(\"‚ùå Visualization failed\")\n",
    "        print(response[\"result\"][\"content\"][0][\"text\"])\n",
    "    else:\n",
    "        print(\"‚úÖ Visualization successful!\")\n",
    "\n",
    "        # Save and display the result\n",
    "        if \"content\" in response[\"result\"] and len(response[\"result\"][\"content\"]) > 0:\n",
    "            content = response[\"result\"][\"content\"][0]\n",
    "            if content[\"type\"] == \"image\" and \"svg\" in content.get(\"mimeType\", \"\"):\n",
    "                svg_data = content[\"data\"]\n",
    "\n",
    "                # Save to file\n",
    "                with open(\"temp/custom_output.svg\", \"w\") as f:\n",
    "                    f.write(svg_data)\n",
    "\n",
    "                # Display\n",
    "                from IPython.display import SVG, display\n",
    "\n",
    "                display(SVG(data=svg_data))\n",
    "\n",
    "                # Show metadata\n",
    "                if \"structuredContent\" in response[\"result\"]:\n",
    "                    metadata = response[\"result\"][\"structuredContent\"].get(\"metadata\", {})\n",
    "                    print(\"\\nüìä Metadata:\")\n",
    "                    print(f\"   Pattern: {metadata.get('pattern_id')}\")\n",
    "                    print(f\"   Template: {metadata.get('template_id')}\")\n",
    "                    if metadata.get(\"processing_time_ms\"):\n",
    "                        print(f\"   Processing time: {metadata['processing_time_ms']}ms\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Stop the vLLM server when done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the vLLM server\n",
    "try:\n",
    "    server_process.terminate()\n",
    "    server_process.wait(timeout=5)\n",
    "    print(\"‚úÖ vLLM server stopped\")\n",
    "except:\n",
    "    print(\"Server process was not running or already stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "1. **No GPU available**: Make sure you've selected GPU runtime (Runtime -> Change runtime type -> GPU)\n",
    "2. **Out of memory**: The A100 40GB should be sufficient, but if you get OOM errors, try restarting the runtime\n",
    "3. **Model download slow**: First run downloads ~14GB model. This is normal and will be cached for future runs\n",
    "4. **Server not starting**: Check the server logs in the cell output for specific errors\n",
    "5. **Connection refused**: Make sure the vLLM server cell is still running"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

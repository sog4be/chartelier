{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chartelier Test with Small Models on Google Colab\n",
    "\n",
    "This notebook tests Chartelier with smaller, faster-loading models for quick validation.\n",
    "\n",
    "## Prerequisites\n",
    "- Google Colab (Free tier is sufficient)\n",
    "- GPU runtime enabled (any GPU type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Clone Repository and Quick Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the Chartelier repository\n",
    "!git clone https://github.com/sog4be/chartelier.git\n",
    "%cd chartelier\n",
    "\n",
    "# Check current branch\n",
    "!git checkout feature/gpt-oss-20b-colab-support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick setup - install only essential dependencies\n",
    "!pip install -q 'numpy<2.0'\n",
    "!pip install -q vllm\n",
    "!pip install -q transformers accelerate\n",
    "!pip install -q -e .\n",
    "\n",
    "print(\"✅ Dependencies installed\")\n",
    "\n",
    "# Verify vLLM\n",
    "import vllm\n",
    "\n",
    "print(f\"vLLM version: {vllm.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Start vLLM Server with Small Model\n",
    "\n",
    "We'll use **Qwen/Qwen2-0.5B-Instruct** - a tiny but capable model that loads quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start vLLM server with small model\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "from threading import Thread\n",
    "\n",
    "# Kill any existing server\n",
    "!pkill -f vllm.entrypoints.openai.api_server || true\n",
    "\n",
    "# Server configuration\n",
    "MODEL = \"Qwen/Qwen2-0.5B-Instruct\"  # Small 0.5B parameter model\n",
    "# Alternative options:\n",
    "# MODEL = \"facebook/opt-125m\"  # Even smaller (125M)\n",
    "# MODEL = \"microsoft/phi-2\"  # Small but powerful (2.7B)\n",
    "\n",
    "\n",
    "def start_server():\n",
    "    cmd = [\n",
    "        \"python\",\n",
    "        \"-m\",\n",
    "        \"vllm.entrypoints.openai.api_server\",\n",
    "        \"--model\",\n",
    "        MODEL,\n",
    "        \"--host\",\n",
    "        \"0.0.0.0\",\n",
    "        \"--port\",\n",
    "        \"8000\",\n",
    "        \"--max-model-len\",\n",
    "        \"2048\",\n",
    "        \"--dtype\",\n",
    "        \"half\",\n",
    "        \"--trust-remote-code\",\n",
    "    ]\n",
    "    subprocess.run(cmd)\n",
    "\n",
    "\n",
    "# Start server in background thread\n",
    "server_thread = Thread(target=start_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(f\"🚀 Starting vLLM server with {MODEL}...\")\n",
    "print(\"⏳ This should take less than 1 minute...\")\n",
    "\n",
    "# Wait for server to be ready\n",
    "max_wait = 60  # 1 minute max\n",
    "for i in range(max_wait):\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:8000/health\", timeout=1)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"\\n✅ Server is ready! (took {i + 1} seconds)\")\n",
    "            break\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if i % 10 == 9:\n",
    "        print(f\"Still waiting... ({i + 1}s)\")\n",
    "    time.sleep(1)\n",
    "else:\n",
    "    print(\"❌ Server failed to start within 60 seconds\")\n",
    "\n",
    "# Check if model is loaded\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:8000/v1/models\")\n",
    "    if response.status_code == 200:\n",
    "        models = response.json().get(\"data\", [])\n",
    "        if models:\n",
    "            print(f\"✅ Model loaded: {models[0]['id']}\")\n",
    "except:\n",
    "    print(\"⚠️ Could not verify model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test the Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test of the server\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://localhost:8000/v1/chat/completions\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "data = {\n",
    "    \"model\": MODEL,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n",
    "    ],\n",
    "    \"temperature\": 0.0,\n",
    "    \"max_tokens\": 50,\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(url, headers=headers, json=data, timeout=10)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(\"✅ Server test successful!\")\n",
    "        print(f\"Response: {result['choices'][0]['message']['content']}\")\n",
    "    else:\n",
    "        print(f\"❌ Server error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to connect: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configure Environment for Chartelier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "import os\n",
    "\n",
    "os.environ[\"CHARTELIER_LLM_MODEL\"] = MODEL\n",
    "os.environ[\"CHARTELIER_LLM_API_BASE\"] = \"http://localhost:8000/v1\"\n",
    "os.environ[\"CHARTELIER_LLM_API_KEY\"] = \"dummy\"\n",
    "os.environ[\"CHARTELIER_LLM_TIMEOUT\"] = \"30\"\n",
    "\n",
    "print(\"✅ Environment configured:\")\n",
    "print(f\"   Model: {MODEL}\")\n",
    "print(f\"   API Base: http://localhost:8000/v1\")\n",
    "print(f\"   Timeout: 30s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Chartelier Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chartelier test\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, \"/content/chartelier/src\")\n",
    "\n",
    "from chartelier.interfaces.mcp.handler import MCPHandler\n",
    "from chartelier.interfaces.mcp.protocol import JSONRPCRequest, MCPMethod\n",
    "from chartelier.infra.llm_client import LLMSettings\n",
    "\n",
    "\n",
    "def test_chartelier():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🧪 Chartelier Test with Small Model\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Configuration\n",
    "    settings = LLMSettings()\n",
    "    print(f\"\\n✅ Configuration:\")\n",
    "    print(f\"   Model: {settings.model}\")\n",
    "    print(f\"   API Base: {settings.api_base}\")\n",
    "    print(f\"   Timeout: {settings.timeout}s\")\n",
    "\n",
    "    # Create handler\n",
    "    handler = MCPHandler()\n",
    "    print(\"✅ MCP handler created\")\n",
    "\n",
    "    # Simple test data\n",
    "    csv_data = \"\"\"month,sales,category\n",
    "2024-01,1000,Product A\n",
    "2024-02,1200,Product A\n",
    "2024-03,1100,Product A\n",
    "2024-04,1300,Product A\n",
    "2024-01,800,Product B\n",
    "2024-02,900,Product B\n",
    "2024-03,950,Product B\n",
    "2024-04,1050,Product B\"\"\"\n",
    "\n",
    "    # Create request\n",
    "    request = JSONRPCRequest(\n",
    "        id=1,\n",
    "        method=MCPMethod.TOOLS_CALL,\n",
    "        params={\n",
    "            \"name\": \"chartelier_visualize\",\n",
    "            \"arguments\": {\n",
    "                \"data\": csv_data,\n",
    "                \"query\": \"Show monthly sales trends for Product A and Product B as a line chart\",\n",
    "                \"options\": {\n",
    "                    \"format\": \"svg\",\n",
    "                    \"width\": 800,\n",
    "                    \"height\": 600,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(\"\\n✅ Request prepared\")\n",
    "    print(f\"📈 Data: {len(csv_data.splitlines()) - 1} rows\")\n",
    "    print(f\"📝 Query: 'Show monthly sales trends'\")\n",
    "    print(f\"🎨 Format: SVG (800x600)\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"⚠️  Using small model: {MODEL}\")\n",
    "    print(\"Note: Results may vary with smaller models\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\n⏳ Processing visualization request...\")\n",
    "\n",
    "    try:\n",
    "        response_str = handler.handle_message(json.dumps(request.model_dump()))\n",
    "        response = json.loads(response_str)\n",
    "\n",
    "        if response.get(\"result\", {}).get(\"isError\"):\n",
    "            print(\"\\n❌ Visualization failed\")\n",
    "            error_msg = response[\"result\"][\"content\"][0][\"text\"]\n",
    "            print(f\"   Error: {error_msg}\")\n",
    "\n",
    "            if \"structuredContent\" in response[\"result\"]:\n",
    "                error = response[\"result\"][\"structuredContent\"].get(\"error\", {})\n",
    "                print(f\"   Code: {error.get('code')}\")\n",
    "                if error.get(\"hint\"):\n",
    "                    print(f\"   Hint: {error.get('hint')}\")\n",
    "            return None\n",
    "        else:\n",
    "            print(\"\\n✅ Visualization successful!\")\n",
    "\n",
    "            result = response[\"result\"]\n",
    "\n",
    "            # Check content\n",
    "            if \"content\" in result and len(result[\"content\"]) > 0:\n",
    "                content = result[\"content\"][0]\n",
    "                if content[\"type\"] == \"image\":\n",
    "                    print(f\"\\n📊 Chart generated:\")\n",
    "                    print(f\"   MIME type: {content.get('mimeType', 'unknown')}\")\n",
    "                    print(f\"   Data size: {len(content.get('data', ''))} characters\")\n",
    "\n",
    "                    # Show metadata\n",
    "                    if \"structuredContent\" in result and \"metadata\" in result[\"structuredContent\"]:\n",
    "                        metadata = result[\"structuredContent\"][\"metadata\"]\n",
    "                        print(f\"\\n📊 Metadata:\")\n",
    "                        print(f\"   Pattern: {metadata.get('pattern_id')}\")\n",
    "                        print(f\"   Template: {metadata.get('template_id')}\")\n",
    "\n",
    "                        if metadata.get(\"stats\", {}).get(\"duration_ms\"):\n",
    "                            duration = metadata[\"stats\"][\"duration_ms\"]\n",
    "                            total = duration.get(\"total\", 0)\n",
    "                            print(f\"   Processing time: {total:.0f}ms\")\n",
    "\n",
    "                    # Return SVG\n",
    "                    if \"svg\" in content.get(\"mimeType\", \"\"):\n",
    "                        return content[\"data\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Unexpected error: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# Run test\n",
    "svg_data = test_chartelier()\n",
    "\n",
    "# Display result\n",
    "if svg_data:\n",
    "    from IPython.display import SVG, display\n",
    "\n",
    "    print(\"\\n📊 Displaying generated chart:\")\n",
    "    display(SVG(data=svg_data))\n",
    "\n",
    "    # Save to file\n",
    "    with open(\"/content/output.svg\", \"w\") as f:\n",
    "        f.write(svg_data)\n",
    "    print(\"\\n💾 Chart saved to: /content/output.svg\")\n",
    "else:\n",
    "    print(\"\\n❌ Chart generation failed\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Check if server is running (Step 2)\")\n",
    "    print(\"2. Try a different model\")\n",
    "    print(\"3. Check server logs for errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Models to Try\n",
    "\n",
    "If the above doesn't work, try these alternative models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of small models you can try\n",
    "models = [\n",
    "    \"facebook/opt-125m\",  # 125M - Tiny, fast\n",
    "    \"facebook/opt-350m\",  # 350M - Still small\n",
    "    \"microsoft/phi-2\",  # 2.7B - Small but capable\n",
    "    \"Qwen/Qwen2-0.5B-Instruct\",  # 0.5B - Good balance\n",
    "    \"Qwen/Qwen2-1.5B-Instruct\",  # 1.5B - Better quality\n",
    "]\n",
    "\n",
    "print(\"Available small models for testing:\")\n",
    "for i, model in enumerate(models, 1):\n",
    "    print(f\"{i}. {model}\")\n",
    "\n",
    "print(\"\\nTo use a different model:\")\n",
    "print(\"1. Stop the current server (Runtime → Interrupt)\")\n",
    "print(\"2. Change MODEL variable in Step 2\")\n",
    "print(\"3. Re-run from Step 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the server\n",
    "!pkill -f vllm.entrypoints.openai.api_server || true\n",
    "print(\"✅ Server stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "1. **Server won't start**: Try a smaller model (opt-125m)\n",
    "2. **Low quality results**: Expected with small models - they may struggle with complex reasoning\n",
    "3. **Out of memory**: Restart runtime and use smaller model\n",
    "4. **Connection refused**: Make sure server cell completed successfully\n",
    "\n",
    "### Tips:\n",
    "- Small models work best with simple, clear queries\n",
    "- For production use, larger models (7B+) are recommended\n",
    "- This notebook is for testing Chartelier's functionality, not model quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
